{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da59b26b-5672-4406-81ec-d51c7b2778c6",
   "metadata": {},
   "source": [
    "## LECTURE 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f59458-d682-4b6e-af78-f449303d7fe2",
   "metadata": {},
   "source": [
    "\n",
    "5.1. Mode\n",
    "The outcome having the highest density. That is, for a random variable $X$ with PDF $f_X(x)$:\n",
    "\n",
    "$$\\text{Mode} = {\\arg \\max}_x f_X(x).$$\n",
    "5.2. Entropy\n",
    "The entropy can be defined by replacing the sum in the finite case with an integral:\n",
    "\n",
    "$$H(X) = -\\int_x f_X(x) \\log [f_X(x)] \\text{d}x.$$\n",
    "5.3. Mean\n",
    "The distribution-based definition is\n",
    "\n",
    "$$\\mathbb{E}(X) = \\int_x x \\, f_X(x) \\text{d}x.$$\n",
    "5.4. Variance\n",
    "The distribution-based definition is\n",
    "\n",
    "$$\\text{Var}(X) = \\mathbb{E}[(X - \\mu_X)^2] = \\int_x (x - \\mu_X) ^ 2 \\, f_X(x) \\text{d}x,$$\n",
    "\n",
    "\n",
    "5.6. Quantiles\n",
    "More general than a median is a quantile. The definition of a $p$-quantile $Q(p)$ is the outcome with a probability $p$ of getting a smaller outcome. So, its distribution-based definition satisfies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87277f4e-4a74-4bdb-8d72-7e212746954e",
   "metadata": {},
   "source": [
    "\n",
    "5.7. Prediction Intervals\n",
    "It is often useful to communicate an interval for which a random outcome will fall in with a pre-specified probability $p$. Such an interval is called a $p \\times 100\\%$ <strong>prediction interval</strong>. You can calculate the lower limit of this interval as the $\\frac{1 - p}{2}$-quantile and the upper limit as the $\\frac{1 + p}{2}$-quantile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c160bf0f-5ef3-4f0b-85b3-90d197801913",
   "metadata": {},
   "source": [
    "\n",
    "6.3. Survival Function\n",
    "The survival function $S(\\cdot)$ is the CDF \"flipped upside down\". For the random variable $X$, the survival function is defined as\n",
    "\n",
    "$$S_X(x) = P(X \\gt x) = 1 - F_X(x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e390c47d-f994-4e38-bd68-1eb30525d020",
   "metadata": {},
   "source": [
    "## LECTURE 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5579b769-bd4d-46ae-aac3-da67eb1aa324",
   "metadata": {},
   "source": [
    "This function is called a bivariate density function.\n",
    "\n",
    "Notation: For two random variables $X$ and $Y$, their joint PDF evaluated at the points $x$ and $y$ is usually denoted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15fbd75-17a0-4b44-a539-33fe25e2c20e",
   "metadata": {},
   "source": [
    "2.2. Calculating Probabilities\n",
    "In the bivariate case, since we have a density surface, we can calculate probabilities as the volume under the density surface. This means that the total volume under the density function must equal 1. Formally, this may be written as\n",
    "\n",
    "$$\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f_{X, Y} (x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y = 1$$\n",
    "\n",
    "\n",
    "Note what this implies about the units of $f_{X, Y}(x,y)$. For example, if $x$ is measured in metres and $y$ is measure in seconds, then the units of $f_{X, Y}(x,y)$ are $\\text{m}^{-1} \\text{s}^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6668afb8-9acc-43a9-8aee-cae20c804a01",
   "metadata": {},
   "source": [
    "\n",
    "Â¶\n",
    "To describe this situation, let us use the marathon runners' example again:\n",
    "\n",
    "If Runner 1 ended up finishing in $5.2$ hours, what is the distribution of Runner 2's time?\n",
    "\n",
    "Let $X$ be the time for Runner 1, and $Y$ for Runner 2, what we are asking for is\n",
    "\n",
    "$$f_{Y|X = 5.2}(y).$$\n",
    "However, we already pointed out that\n",
    "\n",
    "$$P(X = 5.2) = 0.$$\n",
    "We end up with\n",
    "\n",
    "$$f_{Y|X = 5.2}(y) = \\frac{f_{Y,X}(y, 5.2)}{f_X(5.2)}.$$\n",
    "This formula is true in general\n",
    "\n",
    "$$f_{Y|X}(y) = \\frac{f_{Y,X}(y, x)}{f_X(x)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1ae21c-d879-4d3f-b92b-37b25fbb189f",
   "metadata": {},
   "source": [
    "## LECTURE 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef461b2-4acc-4c1e-912c-50fe1c6a6802",
   "metadata": {},
   "source": [
    "\n",
    "2. Independence (Revisited)\n",
    "2.1. Definition in the Continuous Case\n",
    "Recall that independence of random variables $X$ and $Y$ means that knowledge about one variable tells us nothing about the other.\n",
    "\n",
    "In the discrete case, this means that a joint probability distribution (when depicted as a table) has each row/column as a multiple of the others, because (by definition of independence):\n",
    "\n",
    "$$P(X = x \\cap Y = y) = P(X = x) \\cdot P(Y = y).$$\n",
    "Or, equivalently,\n",
    "\n",
    "$$P(Y = y \\mid X = x) = P(Y = y).$$\n",
    "In the continuous case, probabilities become densities. A definition of independence becomes\n",
    "\n",
    "$$f_{X,Y}(x, y) = f_X(x) \\cdot f_Y(y),$$\n",
    "where $f_{X,Y}(x, y)$ is the joint PDF of the continuous random variables $X$ and $Y$, $f_X(x)$ is the marginal PDF of $X$, and $f_Y(y)$ is the marginal PDF of $Y$.\n",
    "\n",
    "Or, equivalently,\n",
    "\n",
    "$$f_{Y \\mid X}(y) = f_Y(y).$$\n",
    "Each of these two definitions has an intuitive meaning:\n",
    "\n",
    "The first definition, $f_{X,Y}(x, y) = f_X(x) \\cdot f_Y(y)$, means that, when slicing the joint density at various points along the $x$-axis (and also for the $y$-axis), the resulting one-dimensional function will be the same, except for some multiplication factor.\n",
    "The second definition, $f_{Y \\mid X}(y) = f_Y(y)$, is probably more intuitive. As in the discrete case, it means that knowing $X$ does not tell us anything about $Y$. The same could be said about the reverse. To clarify, we have the continuous version of the conditional probability formula:\n",
    "$$f_{X,Y}(x, y) = f_{Y \\mid X}(y) \\ f_X(x).$$\n",
    "Setting $f_{Y \\mid X}(\\cdot)$ equal to $f_Y(\\cdot)$ results in the original definition from the first bullet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64afe32-c3d0-48ef-b4c5-e9355945db97",
   "metadata": {},
   "source": [
    "$$f_{Y \\mid X}(y) = \\frac{f_{X,Y}(x,y)}{f_X(x)} = \\frac{f_{X}(x) \\cdot f_{Y}(y)}{f_X(x)} = f_Y(y).$$\n",
    "And we have the same for $X \\mid Y$. Again, we are back to the definition of independence!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad1ddc3-5ff6-4bf6-8982-246d55750e26",
   "metadata": {},
   "source": [
    "\n",
    "4.1. Parameters\n",
    "To characterize the bivariate Gaussian family, we need the following parameters:\n",
    "\n",
    "The parameters of the two marginals (mean and variance for both $X$ and $Y$ denoted $\\mu_X, \\mu_Y, \\sigma^2_X, \\sigma^2_Y$), and\n",
    "the covariance between $X$ and $Y$, sometimes denoted $\\sigma_{XY}$ (or, equivalently, the Pearson correlation, sometimes denoted $\\rho$).\n",
    "That is five parameters altogether, and only one of them (Pearson correlation or covariance) is needed to specify the dependence part.\n",
    "\n",
    "Using the parameters of a bivariate Gaussian distribution, we can construct two objects that are useful for computations: a mean vector $\\boldsymbol{\\mu}$ and a covariance matrix $\\boldsymbol{\\Sigma}$, where\n",
    "\n",
    "$$\\boldsymbol{\\mu}=\\begin{pmatrix} \\mu_X \\\\ \\mu_Y \\end{pmatrix}$$\n",
    "and\n",
    "\n",
    "$$\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\sigma_X^2 &amp; \\sigma_{XY} \\\\ \\sigma_{XY} &amp; \\sigma_Y^2 \\end{pmatrix}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0a001b-e26a-46bc-8f4e-82429e38b4f8",
   "metadata": {},
   "source": [
    "correlation and covariance \n",
    "$\\sigma_{XY} = \\rho_{XY}  \\sigma_X  \\sigma_Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a0448-8d75-4ee6-90e0-bfde9415e6eb",
   "metadata": {},
   "source": [
    "In general, the multivariate Gaussian distribution of $d$ variables has some generic $d$-dimensional mean vector and a $d \\times d$ covariance matrix, where the upper-right and lower-right triangles of the covariance matrix are the same.\n",
    "\n",
    "This means that to fully specify this $d$-dimensional distribution, we need:\n",
    "\n",
    "the means and variances of all $d$ random variables, and\n",
    "the covariance or correlations between each pair of random variables, i.e., $d \\choose 2$ of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499df397-4596-4f84-84f9-ee1f2183738f",
   "metadata": {},
   "source": [
    "Here is an example of a covariance matrix with $d = 3$, and random variables $X, Y, Z$:\n",
    "\n",
    "$$\n",
    "\\Sigma=\\begin{pmatrix} \n",
    "   \\sigma_X^2  &amp; \\sigma_{XY} &amp; \\sigma_{XZ} \\\\ \n",
    "   \\sigma_{XY} &amp; \\sigma_Y^2  &amp; \\sigma_{YZ}\\\\ \n",
    "   \\sigma_{XZ} &amp; \\sigma_{YZ} &amp; \\sigma_Z^2 \n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "The correlation matrix with $d = 3$ will be:\n",
    "\n",
    "$$\n",
    "\\mathbf{P} = \\begin{pmatrix} \n",
    "   1 &amp; \\rho_{XY} &amp; \\rho_{XZ} \\\\ \n",
    "   \\rho_{XY} &amp; 1  &amp; \\rho_{YZ}\\\\ \n",
    "   \\rho_{XZ} &amp; \\rho_{YZ} &amp; 1 \n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02abab49-34d3-45c5-a01a-7a17d384740b",
   "metadata": {},
   "source": [
    "\n",
    "4.3. Properties\n",
    "This multivariate Gaussian distribution has the following properties:\n",
    "\n",
    "Marginal distributions are Gaussian. The marginal distribution of a subset of variables can be obtained by just taking the relevant subset of means, and the relevant subset of the covariance matrix.\n",
    "Linear combinations are Gaussian. If $(X, Y)$ have a bivariate Gaussian distribution, then $Z = aX + bY + c$ for constants $a, b, c$ is Gaussian. If we want to find the mean and variance, we apply the linearity of expectations and variance rules:\n",
    "$$\\mathbb{E}(Z) = \\mathbb{E}(aX + bY + c) = a \\mu_X + b \\mu_Y + c,$$\n",
    "and\n",
    "\n",
    "$$\\text{Var}(Z) = \\text{Var}(aX + bY + c) = a^2 \\sigma_X^2 + b^2 \\sigma_Y^2 + 2ab\\sigma_{XY}.$$\n",
    "Conditional distributions are Gaussian. If $(X, Y)$ have a bivariate Gaussian distribution, then the distribution of $Y$ given that $X = x$ is also Gaussian. Its distribution is\n",
    "$$Y \\mid X = x \\sim \\mathcal{N} \\left(\\mu_{_{Y \\mid X = x}} = \\mu_Y + \\frac{\\sigma_Y}{\\sigma_X} \\rho_{XY} (x - \\mu_X), \\sigma^2_{_{Y \\mid X = x}} = \\ (1 - \\rho_{XY}^2)\\sigma_Y^2 \\right).$$\n",
    "Note the following:\n",
    "\n",
    "The conditional mean is linear in $x$ and passes through the mean $(\\mu_X, \\mu_Y)$, and has a steeper slope with higher correlation.\n",
    "The conditional variance is smaller than the marginal variance, and gets smaller with higher correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2283f47-115f-4cc3-95e5-ed51d874b4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
